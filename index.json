[{"authors":null,"categories":null,"content":"Currently I am employed as a research scientist at deCODE genetics, working on cutting edge genetics research. Before I moved back to Iceland I lived in Denmark for six years and the last two years I was employed as a senior software engineer at Oqton, where I worked with awesome talented people building the operating system for intelligent factories.\nPrior I was a postdoctoral researcher at DTU Compute in the Image Analysis and Computer Graphics group. I focused my research on Geometric Deep Learning ( gdl Journal Club). In my PhD I worked primarily on methods and applications in the domain of statistical learning. My PhD thesis is titled Sparse Classification - Methods and Applications. Accompanying my thesis is the R-package accSDA, available on CRAN.\n","date":1609333200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1609333200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://gumeo.github.io/author/gu%C3%B0mundur-einarsson/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/gu%C3%B0mundur-einarsson/","section":"authors","summary":"Currently I am employed as a research scientist at deCODE genetics, working on cutting edge genetics research. Before I moved back to Iceland I lived in Denmark for six years and the last two years I was employed as a senior software engineer at Oqton, where I worked with awesome talented people building the operating system for intelligent factories.","tags":null,"title":"GuÃ°mundur Einarsson","type":"authors"},{"authors":["GuÃ°mundur Einarsson"],"categories":["Blog"],"content":"Disclaimer: I am not an expert on covid, and I am interested in learning more about what the future might hold for us in 2021.\nThis year has been quite eventful (covid-19), but then again, this overextended staycation has also in some sense made it uneventful for many ( good example from a german ad). Covid-19 has been a real reality curveball. A global pandemic is undoubtedly a ticking time bomb, and given the historical rarity of such events, it is not something that you frequently consider. A pandemic is a sporadic event that messes up the economy and causes a global recession. I like speculating about the future, and it is interesting to ponder how 2021 will follow up on these exciting times.\nThe three phases We summarise the fight against the pandemic into three rough phases; the first two are:\n The sudden reality blow and implementation of measures to deal with the spread. The waiting game for the vaccine.  This partition is an oversimplification, but in these terms, we are entering the third phase, the distribution of the vaccine. I find this an exciting phase since the vaccine is probably the most sought after product on this planet. The manufacturers of the vaccine face some extreme challenges in scaling up production and the logistics of transporting the goods.\nCountries need to deal with their vaccine strategies. I have mostly been following the news in Iceland and for the EU. The general trend is to minimize the number of casualties and hospitalizations and to keep the frontline of health care workers protected. There will still be quite some time until whole populations of countries will reach herd immunity since that requires at least 60% of the people to be vaccinated ( lower bound from this Lancet article). The global economy will suffer until we have vaccinated enough people to achieve close to international herd immunity. I do not expect this to happen in 2021, and it might even take a few years since the virus is so widespread and contagious.\nThere are at least two possible scenarios:\n Everyone equally ramps up vaccination, and the whole world reaches herd immunity almost simultaneously. Some countries will reach herd immunity before others.  I think it is safe to say that option one will most certainly not happen. So given that some countries will reach herd immunity well before others, what are the implications? The following are my speculations.\nDifferent vaccine strategies Not all countries will implement the same vaccine strategies. When deciding on what method to pursue, we need to take into consideration how the vaccine works. The effects of the vaccine are twofold.\n Local: Protection of a single individual. Global: Herd immunity.  Most countries will not receive enough vaccine to reach herd immunity fast, so they will need to prioritize the first groups to get vaccinated. In Europe, these are almost exclusively older sensitive individuals and frontline health care workers. This prioritization is a smart approach. Protecting the more sensitive individuals will lower the number of hospitalizations in future waves, and protecting health care workers will make the health care system more robust against the increased spread.\nI have not seen any alternatives to this general trend of strategies. Mirroring the local and global effects of the vaccine, we have two objectives along the way.\n Protect sensitive and weak individuals. Maximize effectiveness to achieve herd immunity.  I argue that these are somewhat opposing objectives. To understand why we will need to consider how a disease like covid-19 spread. The basic reproduction number $R_0$ or R nought is a number representing the rate of spread for a disease. This number is relatively simple to interpret. If the value is 2, then it means that each infected individual will on average infect two others. If the value is less than 1, the spread is declining, and if it is above 1, it is increasing.\nFor covid-19, $R_0$ has been evaluated to be in the range of $3.28-5.7$. Note that lockdown measures will affect $R_0$, also wearing masks and other sanitary standards. The value is thus dependent on the rules and regulations that people follow in general.\nSo $R_0$ is, in this sense, a dynamic value that we can control to mitigate the spread. But remember that it is defined as an average. To consider the bigger picture, we can consider individuals as nodes in a graph, and nodes are connected by a weighted edge if there is some positive likelihood of spread if individuals get infected. Let\u0026rsquo;s call the graph the spread graph. The weight of the edge between two nodes would imply the probability of transmission. Thus there would be high weights between very connected individuals, e.g., within families, and the weight would decrease fast with geographic distance.\nThis very complicated spread graph is what we are changing with all the measures we are taking against covid-19. The measures will either remove edges from the graph or decrease the weights dramatically. To maximize the vaccine\u0026rsquo;s effectiveness in terms of herd immunity, we would ideally find the most connected nodes in this graph and sort them by the number of outwards connections (weighted by the probability of transmission) and start vaccinating these highly connected individuals. These changes would mean that we change the graph by removing the maximum number of edges at a time. However, this scenario is impossible since there is no way for us to get a complete picture of this graph.\nNow it might be clear why the two objectives of protecting sensitive individuals and maximizing progress towards herd immunity could be competing. I.e., if sensitive individuals have few outgoing edges in the spread graph, then vaccinating them will mostly focus on the first objective. It is also impossible to reach herd immunity if we do not have enough of the vaccine, so it is critical to protect sensitive individuals and health care workers.\nWhen we have protected the sensitive individuals, we might start trying to find highly connected individuals, and vaccinating them, e.g., hairdressers, store clerks, fitness trainers, etc. But where do you draw the line of when we have protected the sensitive individuals? This process is a highly ethical matter.\nIncentives to help Some countries will reach herd immunity faster and, in some cases, will have an abundance of the vaccine. They will be incentivized to help other countries to make a recovery for the global economy faster. The countries they choose to help will be the countries they are most dependent on concerning the economy. This selection will be a political debate in some countries since it will not be a clear choice on who should receive their extra doses.\nHere I would like to see a foundation created (or, e.g., Melinda Gates Foundation) to take care of this (which might become a logistic nightmare). Ideally, the extra doses of vaccine should be redistributed as fast as possible, and no political debate should delay it. A centralized non-profit foundation could do wonders here to make the redistribution more effective. This global effort will require an abundance of funds.\nQuarantine zones The countries that reach herd immunity fast will, in some sense, be the safest travel destinations. People traveling from these countries will face minimum restrictions in terms of being quarantined. People moving between countries could also hop to a quarantine zone country and stay there for a week to avoid being quarantined in a hotel room or something similar for a week. These quarantine zones could result in kickstart for global tourism and international passenger flight.\nSmaller populations might reach herd immunity earlier. Having these herd immunity spots worldwide will create value for the global logistics chain and help us understand the vaccines' long-term effectiveness.\nSpeculations My words here are mere speculations; only time can tell how things will eventually pan out. I look forward to better times, and I believe in a bright future.\n","date":1609333200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609333200,"objectID":"f2ea10b3c152779dee46249e123f74e2","permalink":"https://gumeo.github.io/post/speculations-2021/","publishdate":"2020-12-30T13:00:00Z","relpermalink":"/post/speculations-2021/","section":"post","summary":"Disclaimer: I am not an expert on covid, and I am interested in learning more about what the future might hold for us in 2021.\nThis year has been quite eventful (covid-19), but then again, this overextended staycation has also in some sense made it uneventful for many ( good example from a german ad).","tags":["Blog"],"title":"Speculations on 2021","type":"post"},{"authors":["GuÃ°mundur Einarsson"],"categories":["Blog"],"content":"Updating the blog\u0026hellip; I finally moved back to Iceland, and I will probably have some more time to jot down what goes through my mind. I recently started a new position as a research scientist at deCODE genetics, so expect some new posts about genetics and statistics! I hope I will also have time to work more on C++ projects, I think it will probably be in relation to R package development.\nRegarding the blog\u0026hellip; I was lagging very far behind on Hugo version, went from 0.4 to 0.75, and the academic theme had changed a lot, (FYI hugo is a static webpage generator writtin in golang, simple and easy to use). I decided to slim this down into a more simple version of the academic theme, which is essentially just a blog. That suits my needs perfectly. I needed the extended version of hugo for this, I still need to look into what more is in this extended version, but it seems to be some more CSS support.\n","date":1601211600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601211600,"objectID":"dccc92665c3df0eab06fe7349a657ee3","permalink":"https://gumeo.github.io/post/updating-blog/","publishdate":"2020-09-27T13:00:00Z","relpermalink":"/post/updating-blog/","section":"post","summary":"Updating the blog\u0026hellip; I finally moved back to Iceland, and I will probably have some more time to jot down what goes through my mind. I recently started a new position as a research scientist at deCODE genetics, so expect some new posts about genetics and statistics!","tags":["Blog"],"title":"Simplifying blog","type":"post"},{"authors":["GuÃ°mundur Einarsson"],"categories":["Programming"],"content":"tl;dr emplace_back is often mistaken as a faster push_back, while it is in fact just a different tool. Do not blindly replace push_back by emplace_back, be careful of how you use emplace_back, since it can have unexpected consequences.\nI have repeatedly run into the choice of using emplace_back instead of push_back in C++. This short blog post serves as my take on this decision.\nBoth of the methods in the title, along with insert and emplace, are ways to insert data into standard library containers. emplace_back is for adding a single element to the dynamic array std::vector. There is a somewhat subtle difference between the two:\n push_back calls the constructor of the data that you intend to push and then pushes it to the container. emplace_back \u0026ldquo;constructs in place\u0026rdquo;, so one skips an extra move operation, potentially creating faster bytecode. This is done by forwarding the arguments to the container\u0026rsquo;s template type constructor.  On the surface, emplace_back might look like a faster push_back, but there is a subtle difference contained in the act of forwarding arguments. Searching for the problem online yields a lengthy discussion on the issue (emplace_back vs push_back). In summary, the discussion leans towards choosing emplace_back to insert data into your container, however the reason is not completely clear.\nBe careful After searching a bit more I found this post, which stresses how careful one should be. To further stress the ambiguity of the matter, the google c++ style guide does not provide an explicit preference. The reason they don\u0026rsquo;t state a preference, is because these are simply slightly different tools, and you should not use emplace_back unless you properly understand it, and there is a proper reason for it.\nThe following code should make it clear how emplace_back is different from push_back:\n#include\u0026lt;vector\u0026gt; #include\u0026lt;iostream\u0026gt; int main(){ // Basic example std::vector\u0026lt;int\u0026gt; foo; foo.push_back(10); foo.emplace_back(20); // More tricky example std::vector\u0026lt;std::vector\u0026lt;int\u0026gt;\u0026gt; foo_bar; //foo_bar.push_back(10); // Throws error!!!! foo_bar.emplace_back(20); // Compiles with no issue std::cout \u0026lt;\u0026lt; \u0026quot;foo_bar size: \u0026quot; \u0026lt;\u0026lt; foo_bar.size() \u0026lt;\u0026lt; \u0026quot;\\n\u0026quot;; std::cout \u0026lt;\u0026lt; \u0026quot;foo_bar[0] size: \u0026quot; \u0026lt;\u0026lt; foo_bar[0].size() \u0026lt;\u0026lt; \u0026quot;\\n\u0026quot;; return 0; }  Uncommenting the line foo_bar.push_back(10) yields the following compilation error.\n$ g++ test.cpp -o test test.cpp: In function â€˜int main()â€™: test.cpp:11:24: error: no matching function for call to â€˜std::vector\u0026lt;std::vector\u0026lt;int\u0026gt; \u0026gt;::push_back(int)â€™ foo_bar.push_back(10); ^ ... Some more verbose diagnostic  So we get an error. An extra diagnostic provides us with the following: no known conversion for argument 1 from â€˜intâ€™ to â€˜const value_type\u0026amp; {aka const std::vector\u0026lt;int\u0026gt;\u0026amp;}â€™. So it seems there is no conversion here that makes sense.\nThe compilation completes without errors if we comment out the trouble line. Running the code yields the following result:\n$ ./test foo_bar size: 1 foo_bar[0] size: 20  What is the difference? For emplace_back we forward the arguments to the constructor, adding a new std::vector\u0026lt;int\u0026gt; new_vector_to_add(20) to foo_bar. This is the critical difference. So if you are using emplace_back you need to be a bit extra careful in double checking types.\nAnother example with conversion The example above is very simple and shows the difference of forwarding arguments to the value type constructor or not. The following example I added in an SO question shows a bit more subtle case where emplace_back could make us miss catching a narrowing conversion from double to int.\n#include \u0026lt;vector\u0026gt; class A { public: explicit A(int /*unused*/) {} }; int main() { double foo = 4.5; std::vector\u0026lt;A\u0026gt; a_vec{}; a_vec.emplace_back(foo); // No warning with Wconversion //A a(foo); // Gives compiler warning with Wconversion as expected }  Why is this a problem? The problem is that we are unaware of the problem at compile-time. If this was not the intended behavior, we have caused a runtime error, which is generally harder to fix. Let us catch the issue somehow. You might wonder that some warning flags, e.g., -Wall could reveal the issue. However, the program compiles fine with -Wall. -Wall contains narrowing, but it does not contain conversion. Further, adding -Wconversion yields no warnings.\n$ g++ -Wall -Wconversion test_2.cpp -o test  The problem is that this conversion is happening in a system header, so we also need -Wsystem-headers to catch the issue.\n$ g++ -Wconversion -Wsystem-headers test_2.cpp In file included from /usr/include/c++/7/vector:60:0, from test_2.cpp:1: /usr/include/c++/7/bits/stl_algobase.h: In function â€˜constexpr int std::__lg(int)â€™: /usr/include/c++/7/bits/stl_algobase.h:1001:44: warning: conversion to â€˜intâ€™ from â€˜long unsigned intâ€™ may alter its value [-Wconversion] { return sizeof(int) * __CHAR_BIT__ - 1 - __builtin_clz(__n); } ^ /usr/include/c++/7/bits/stl_algobase.h: In function â€˜constexpr unsigned int std::__lg(unsigned int)â€™: /usr/include/c++/7/bits/stl_algobase.h:1005:44: warning: conversion to â€˜unsigned intâ€™ from â€˜long unsigned intâ€™ may alter its value [-Wconversion] { return sizeof(int) * __CHAR_BIT__ - 1 - __builtin_clz(__n); } ^ In file included from /usr/include/x86_64-linux-gnu/c++/7/bits/c++allocator.h:33:0, from /usr/include/c++/7/bits/allocator.h:46, from /usr/include/c++/7/vector:61, from test_2.cpp:1: /usr/include/c++/7/ext/new_allocator.h: In instantiation of â€˜void __gnu_cxx::new_allocator\u0026lt;_Tp\u0026gt;::construct(_Up*, _Args\u0026amp;\u0026amp; ...) [with _Up = A; _Args = {double\u0026amp;}; _Tp = A]â€™: /usr/include/c++/7/bits/alloc_traits.h:475:4: required from â€˜static void std::allocator_traits\u0026lt;std::allocator\u0026lt;_Tp1\u0026gt; \u0026gt;::construct(std::allocator_traits\u0026lt;std::allocator\u0026lt;_Tp1\u0026gt; \u0026gt;::allocator_type\u0026amp;, _Up*, _Args\u0026amp;\u0026amp; ...) [with _Up = A; _Args = {double\u0026amp;}; _Tp = A; std::allocator_traits\u0026lt;std::allocator\u0026lt;_Tp1\u0026gt; \u0026gt;::allocator_type = std::allocator\u0026lt;A\u0026gt;]â€™ /usr/include/c++/7/bits/vector.tccðŸ’¯30: required from â€˜void std::vector\u0026lt;_Tp, _Alloc\u0026gt;::emplace_back(_Args\u0026amp;\u0026amp; ...) [with _Args = {double\u0026amp;}; _Tp = A; _Alloc = std::allocator\u0026lt;A\u0026gt;]â€™ test_2.cpp:9:25: required from here /usr/include/c++/7/ext/new_allocator.h:136:4: warning: conversion to â€˜intâ€™ from â€˜doubleâ€™ may alter its value [-Wfloat-conversion] { ::new((void *)__p) _Up(std::forward\u0026lt;_Args\u0026gt;(__args)...); }  And there you have it - look at all the verbose output. The problem is not apparent from this wall of text for the uninitiated.\nSo when should you use emplace_back? One reason to use emplace_back is when the move operation that we can save, is actually quite expensive. Consider the following example:\nclass Image { Image(size_t w, size_t h); }; std::vector\u0026lt;Image\u0026gt; images; images.emplace_back(2000, 1000);  Instead of moving the image, which consists of potentially a lot of data, we construct it in place, and forward the constructor arguments in order to do that. These kind of cases are quite special, we should already be aware that this is large data that we are adding to the container.\nAnother case for using emplace_back was added in C++17. Then emplace_back returns a reference to the inserted element, this is not possible at all with push_back.\nemplace_back is a potential premature optimization. Going from push_back to emplace_back is a small change that can usually wait, and like the image case, it is usually quite apparent when we want to use it. If you want to use emplace_back from the start, then make sure you understand the differences. This is not just a faster push_back. For safety, reliability, and maintainability reasons, it is better to write the code with push_back when in doubt. This choice reduces the chance of pushing an unwanted hard to find implicit conversion into the codebase, but you should weigh that risk against the potential speedups, these speedups should then ideally be evaluated when profiling.\n","date":1589202000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589202000,"objectID":"9b1a9a5785968ab144c7796913d00b93","permalink":"https://gumeo.github.io/post/emplace-back/","publishdate":"2020-05-11T13:00:00Z","relpermalink":"/post/emplace-back/","section":"post","summary":"tl;dr emplace_back is often mistaken as a faster push_back, while it is in fact just a different tool. Do not blindly replace push_back by emplace_back, be careful of how you use emplace_back, since it can have unexpected consequences.","tags":["cpp"],"title":"emplace_back vs push_back","type":"post"},{"authors":["GuÃ°mundur Einarsson"],"categories":["diet"],"content":"I have to lose weight The reasons I am going through some weightloss has some history behind it, so I\u0026rsquo;m going to briefly outline that.\nKnee Injury End of July 2017 I was extremely unlucky and had an accident in BJJ, where I fully ruptured the PCL, LCL and popliteus tendon (where it connects to the fibula) in my left knee. We have four ligaments in the knee, one on each lateral side, and then the front (ACL) and back (PCL) cross ligaments. The stability of the knee relates to the elasticity (and existence) of these ligaments, but also the muscles around the knee joint. So half of the ligaments and a muscle tendon were torn, ouch!\nThe PCL ligament is the worst to be gone of these. It prevents the lower leg to shift backwards. The doctors measure the difference in posterior shift, compared to the healthy knee. My lower leg was moving backwards 1cm more than the other leg. When I was sitting down and pressed on the front of my knee, I could feel immense difference in slack. The doctors started with a conservative treatment, but I ended up needing a surgery, which was performed in May 2018 ( Picture after surgery, pretty big scar).\nPhysio Training and Diet The surgery fixed most of the difference in slack, but it takes a long time to recover from it. Now I\u0026rsquo;m almost a year post-surgery, and I have been in some pretty intense physio training to try to get the leg as strong as the healthy one. I still haven\u0026rsquo;t started running, but one of the reasons for that is that running has a high impact on the knee joint, which scales with weight. My physical therapist said that when you run, the impact on the knee is four times the bodyweight. Not sure about the exact numbers, but it is definitaly true that the impact scales with weight. So losing weight is a crucial part of the recovery process, such that I can hopefully get back to some of the sport activities I was doing prior to the accident.\nIn terms of BMI, I was overweight (BMI 29), when I had the accident. My fat percentage was around 15%, so I was in pretty good shape. Note that BMI was designed to measure malnutrition, and doesn\u0026rsquo;t take fat percentage into account, so quite a lot of athletes and people with more muscles will be measured as overweight, although they are in ok shape. After the surgery I had to use crutches and was not allowed to put weight on the leg for the first 12 weeks. I gained weight and at the end of the summer 2018, when I started walking, my BMI was 30.5. I was in the obese range, I had not been training anything intense for a year, so it was time to do something if I ever wanted to do sports again.\nCalorie Restriction I started with a sustainable solution, basically reducing the amount of calories I was consuming. I used Myfitnesspal, to track calories and my weight. Myfitnesspal has a built in barcode scanner, with a huge user-generated data-base behind it. So by scanning barcodes of the food packaging, it becomes fast and easy to track the amount you are eating, and macros. I almost never scanned something that had not been logged by a prior user. Myfitnesspal is definitely sitting on a goldmine of consumer data. The only problem this has is when you go eat out, then you cannot barcode scan things, and it becomes a bit more guesswork. I did the scanning for about a month, and that was enough to become aware of portion sizes for the most common things I was consuming.\nThen, the 1st of November I started a new job, and I lost focus on the calorie counting. I felt the weight was slowly creeping back up, so around Christmas I decided that I should try Huel again. I had tried it for a week prior, but I decided that I wanted to commit to it for more than a month to see if it was doable.\nHuel Huel is a very inexpensive meal substitute and it gives you absolutely full control of how many calories you consume. They state that it is a full meal substitute, meaning that it contains all the essential vitamins, minerals and nutrition you need. It is also super convenient, because it is just powder that you mix with water, so the time preparing food is gone, and it takes faster to eat/drink.\nSo that were the pros of Huel. I\u0026rsquo;m always up for trying to do things in a more efficient way, but after trying this I\u0026rsquo;m not sure replacing this with every meal is a good idea. After the first day on huel, you miss chewing stuff. Chewing gum helped a bit, but it is still not a substitute. I read online and some people complained about stomach problems, I experienced it a bit, but not to an extent that concerned me. I\u0026rsquo;m also not sure if Huel contains all the essential vitamins. My nails became stiffer and more brittle, that did not happen when I was just restricting calories. There is also a lot less sodium in Huel compared to the other food I was eating, so I had a big weight drop in the beginning, because I was bloated from the salty food. I would not say I was hungry when on Huel, but I had to go to bed before 10, otherwise I became hungry and it was harder to fall alsleep.\nEating with other people also becomes a bit weird when you bring out your shaker. It was ok at work, but at home I sincerely missed eating dinner with my wife and son, and that complicated the experience a bit. I think I will never try Huel again as a replace every meal, but potentially as breakfast/lunch substitute.\nThe Data But I made it, I tried Huel for more than a month, except was a couple of Saturday nights we went to a restaurant. Almost since I started the weightloss journey, I weight myself everyday. I always did it in the morning, when I woke up. I wanted to see the trend, and what kind of daily variation I could expect. When I tried to get the data from Myfitnesspal I was hit with a paywall! What? do I need to pay to export my data in csv format? But of course someone had created a python module to do this. It mas a matter of pip install myfitnesspal and I had the data in a breeze, doing something similar to this. I made a simple visualisation to compare the three periods, calorie restriction, not paying attention to what I eat (New Work) and of course Huel. The following is a visualization demonstrating that.\nWhat Did I learn? There are higher residuals in the middle period when I am not paying attention to what I eat. I think this is because of the high variation in sodium content, i.e. if I eat fast-food, it is bound to have a lot of salt in it. The two large spikes in the Huel period are the same thing. The prior one is the morning after I went to an all you can eat sushi place, and the other spike is also after going to an asian cuisine restaurant with salty soy sauce.\nNotice that the residuals are pretty big. You need to look at a trend of at least two weeks to see if things are working. Be patient and consistent, that is key. If you are trying something similar, then do not obsess over your daily numbers. Just measure the weight and look at it every two weeks to see if there is a trend.\nAnother thing I learned is that if I want to reach my weight goal of 82 kg, I cannot be lazy about what I eat, the middle period shows that. It might play into it that the middle period covers December, causing an abnormal increase in calorie consumption, (because of Christmas parties), but at least the trend is not going down, so I will have to focus to reach my goals, this is an effort.\nAlso, if I want to reach my goal and maintain it, I have to be consistent. I will continue to track my weight after I reach my goal to make sure I don\u0026rsquo;t slip again. Also, I do not see Huel as an option for consistency. I would maybe consider it for breakfast or lunch in between, but I would rather enjoy a cheewy dinner with my family. If you want to lose weight and stay healthy, it is a matter of choosing a healthier lifestyle, not investing in a fad.\nThe slope for using Huel is slightly more agressive than the calorie restriction, but in the long term, calorie restriction is a more sustainable option, so I will continue with that.\nAppendix I used theme_Publication for the plot from here. This theme makes the plot look a bit cleaner compared to the default ggplot, imo.\nCode for plot library(ggplot2) setwd(\u0026quot;~/Documents/myfitnesspal_data\u0026quot;) source('./plot_funcs.R') data \u0026lt;- read.csv2('./weight_data.csv', sep=\u0026quot; \u0026quot;, stringsAsFactors = FALSE) data \u0026lt;- data[data$Weight != 'None', ] data$Weight \u0026lt;- as.numeric(data$Weight) data$Date \u0026lt;- as.Date(data$Date) data$Type \u0026lt;- factor(c(rep('Calorie Restriction',42), rep('New Work',56), rep('Huel',44)), levels = c(\u0026quot;Calorie Restriction\u0026quot;, \u0026quot;New Work\u0026quot;, \u0026quot;Huel\u0026quot;)) ############################################################################# # Visualisation ############################################################################# p \u0026lt;- ggplot(data, aes(x = Date, y=Weight, group = Type)) + geom_line(aes(group = Type, col=Type))+ geom_smooth(aes(group = Type), method = \u0026quot;lm\u0026quot;, se = FALSE) + labs(title = \u0026quot;Huel vs Cal Restriction \\nFall 2018 to Spring 2019\u0026quot;, x=\u0026quot;Date\u0026quot;, y=\u0026quot;Weight (Kg)\u0026quot;) + scale_colour_Publication()+ theme_Publication() + theme(legend.key.width = unit(3,\u0026quot;cm\u0026quot;))  ","date":1550926800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550926800,"objectID":"0140bc8a3f5b9453bf629ff2bcb50baf","permalink":"https://gumeo.github.io/post/weight-loss/","publishdate":"2019-02-23T13:00:00Z","relpermalink":"/post/weight-loss/","section":"post","summary":"I have to lose weight The reasons I am going through some weightloss has some history behind it, so I\u0026rsquo;m going to briefly outline that.\nKnee Injury End of July 2017 I was extremely unlucky and had an accident in BJJ, where I fully ruptured the PCL, LCL and popliteus tendon (where it connects to the fibula) in my left knee.","tags":["me"],"title":"Eating Less vs Fad? Diets","type":"post"},{"authors":["GuÃ°mundur Einarsson"],"categories":["Academia"],"content":"The choices we make! One of the major decisions that has haunted me through my PhD is whether I want to stay in academia or not. I sincerely love doing research, I love going to conferences, I love reading scientific papers and talking to other scholars. I even love giving talks and teaching, dissemination can be very rewarding and enjoyable. At some point I was certain that I would go for an academic career, but things change.\nAnother thing I like is a fresh challenge. After having spent the summer at deCODE genetics, I saw the possibilities outside academia. Doing a PhD is a binary process, either you have a PhD or not (Quote from my friend Ã“li PÃ¡ll, who recited Professor Havard Rue from NTNU). Having obtained my PhD made me think differently about my own future, and what I could accomplish. We sometimes say, \u0026ldquo;it is the journey, not the destination\u0026rdquo;, I feel that doing a PhD is different, there is a certain destination, obtaining the degree! In that regard I did not allow myself to ponder other possibilities.\nIt is good to evaluate your goals and values every now and then. I have certain goals that I would like to achieve 5, 10 and 20 years down the line. From my point of view, I have a higher likelihood of achieving these goals outside academia, at least if I stay there for some time. I would also feel good about returning to academia at some point, having experienced the outside world.\nOne of the important things my PhD supervisor made me realize is that the most valuable resource we possess is time. Time is a resource that we can never get back. We should use this resource to do something we believe in, do something that makes a difference and we enjoy. I am forever grateful for my experience and time at DTU, I will cherish the moments I had there for the rest of my life. But for me the time for a change is now, it is time to go somewhere else, meet new people, learn, grow and contribute to great things. I look forward to this new path and I am happy for the choice I made.\n","date":1540299600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540299600,"objectID":"37dc9bfc0b48e5d84222c9ced59235d2","permalink":"https://gumeo.github.io/post/leaving-academia/","publishdate":"2018-10-23T13:00:00Z","relpermalink":"/post/leaving-academia/","section":"post","summary":"The choices we make! One of the major decisions that has haunted me through my PhD is whether I want to stay in academia or not. I sincerely love doing research, I love going to conferences, I love reading scientific papers and talking to other scholars.","tags":["Academic"],"title":"Leaving Academia","type":"post"},{"authors":["GuÃ°mundur Einarsson"],"categories":["Programming"],"content":"Let\u0026rsquo;s go on! If you just arrived, you can check out the first part here. The goal of this series is to demonstrate how compactly we can implement an MLP in a functional programming paradigm and how easy it becomes to extend/play around with it. This post is aimed at the R person that wants to get into deep learning or anyone curious about how these things are implemented. Another goal is to visualize what is happening in a neural network during training in hope to get a deeper understanding of what is going on. I posted this gif on the subreddit MachineLearning and dataisbeautiful, I got some constructive criticism on the gif and below is a better version where I have sped it up and removed the flickering and enhanced the contrast.\n   So last time we got away with less than 50 lines of code to create a factory for generating fully connected layers. This included a closure for a matrix class, (for making the layer have a mutable state), and an implementation of a forward pass. But the forward pass is basically a wrapper for updating the internals of the layer, we need to implement a forward pass for the whole network, and the same goes for the backwards propagation. This requires creating a function factory that constructs the network environment. This environment needs to have functions that implement forward-, backwards passes and a function for training. The following is a demonstration of such a function:\n# The following creates an MLP environment # structNet is a vector defining the number of nodes in each layer, ignoring biases. mlp \u0026lt;- function(structNet, minibatchSize,activation, initPos =FALSE, initScale=100){ num_layers \u0026lt;- length(structNet) # Create the network layers \u0026lt;- list() for(i in 1:num_layers){ if(i == 1){ # Input layer layers[[i]] \u0026lt;- Layer(activation, minibatchSize, c(structNet[1]+1,structNet[2]), is_input=TRUE, initPos = initPos, initScale=initScale) }else if(i == length(structNet)){ # Output layer layers[[i]] \u0026lt;- Layer(softmax, minibatchSize, c(structNet[num_layers],structNet[num_layers]), is_output=TRUE, initPos = initPos, initScale=initScale) }else{ # Hidden layers layers[[i]] \u0026lt;- Layer(activation, minibatchSize, c(structNet[i]+1,structNet[i+1]), initPos = initPos, initScale=initScale) } } forward_prop \u0026lt;- function(dataBatch){ # Add bias to the input layers[[1]]$Z$setter(cbind(dataBatch,rep(1,nrow(dataBatch)))) for(i in 1:(num_layers-1)){ layers[[i+1]]$S$setter(layers[[i]]$forward_propagate()) } return(layers[[num_layers]]$forward_propagate()) } backwards_prop \u0026lt;- function(yhat,labels){ layers[[num_layers]]$D$setter(t(yhat-labels)) for(i in (num_layers-1):2){ W_nobias \u0026lt;- layers[[i]]$W$getter() W_nobias \u0026lt;- W_nobias[1:(nrow(W_nobias)-1),] mat \u0026lt;- layers[[i]]$Fp$getter() layers[[i]]$D$setter((W_nobias%*%layers[[i+1]]$D$getter())*mat) } } update_weights \u0026lt;- function(eta){ for(i in 1:(num_layers-1)){ W_grad \u0026lt;- -eta*t(layers[[i+1]]$D$getter()%*%layers[[i]]$Z$getter()) layers[[i]]$W$setter(layers[[i]]$W$getter()+W_grad) } } # Labels here as dummy matrix train \u0026lt;- function(trainData, trainLabels, num_epochs, eta, cvSplit = 0.3){ # Code for preparing input for(i in 1:num_epochs){ # Loop over epochs for(j in 1:numIter){ # Loop over all minibatches # ... # Essential part for training preds \u0026lt;- forward_prop(tDat) backwards_prop(preds,tLab) update_weights(eta = eta) } } } myEnv \u0026lt;- list2env(list(network=layers, forward_prop=forward_prop, train = train)) class(myEnv) \u0026lt;- 'mlp' return(myEnv) }  The mlp function above starts by constructing the network given the layer configuration that we desire. Most of the implementation of the train function is omitted to highlight the essential part. The shown part highlights what is needed for stochastic gradient descent. We need to forward propagate the data, then back propagate the error, and finally based on this error we can update the weights. This is the heart and soul of SGD. Instead of using the whole dataset to estimate the gradient, we subsample a minibatch and estimate the gradient as an average over these samples. Quite a lot of gradient descent based optimization is done as a full batch optimization, specifically when people first learn about it, because SGD just adds an extra layer of complication. SGD allows for online updates, as more data arrives, and it may be computationally more efficient. The only issue is of course the diffculties of scaling the gradient, i.e. choosing the best learning rate, which is also an issue for regular/full battch gradient descent.\nSo the training function is essentially broken up into 3 steps, these steps are abstracted from the training algorithm, so we could easily change the update_weights function to include a momentum term or other things we might want to try, without changing the train function.\nNotice that we take the vector structNet as input. This design can be extended to take in another vector describing what kind of layers we want and build the network with these different layers. This would generalize the implementation quite significantly. This would of course also require us to change the definition of the Layer function, or implement new layer functions.\nIs deep learning simple? Now we have an mlp implementation in roughly 100 lines of code. I think this demonstrates in some way how simple an mlp really is, but also in contrast the difficulties needed to get to the modern implementations of deep learning. Modern implementations are much more general, where they usually implement a computation graph where all computations can be automatically differentiated. These are also made to target GPU hardware, where matrix-matrix and matrix-vector multiplication can be significantly accelerated. There is a package for R called gpuR, where one can do matrix calculations on the GPU. I might try that in the future, to see if I can speed this up significantly with minimal change of this code.\nHow to use this? First we need to define the activation function we want, the softmax function and a function to create a dummy encoding of factors variables in R. I have now included all of these in the package minstr available here. So you do not need to define the yourself. Although you can of course define your own activation function as you wish. Take a look at the following to get an idea of how it is done.\n# rectified linear unit activation # activation functions need a boolean parameter for # calculating the derivative reLU \u0026lt;- function(X,deriv=FALSE){ if(deriv==FALSE){ X[X\u0026lt;0] \u0026lt;- 0 return(X) }else{ X[X\u0026lt;0] \u0026lt;- 0 X[X\u0026gt;0] \u0026lt;- 1 return(X) } } # softmax for output layer softmax \u0026lt;- function(X){ Z \u0026lt;- rowSums(exp(X)) X \u0026lt;- exp(t(X))%*%diag(1/Z) return(t(X)) } # Function to represent factor as dummy matrix class.ind \u0026lt;- function(cl) { Ik=diag(length(levels(cl))) x=Ik[as.numeric(cl),] dimnames(x) \u0026lt;- list(names(cl), levels(cl)) x }  Here is an example of how we can train a network with this. Feel free to play around with some of the parameters, such as the learning rate. If it is too big, nothing will be learned. The current configuration for the network works fine, but you can also try to squeeze it down or remove/add more hidden layers.\n# Load mnist dataset, note that you need to download it first # mnistr::download_mnist() # This saves to the current working directory mnistr::load_mnist() # Visualize random digit, just to get familiar with the data, in case this is the # first time you see mnist randDig \u0026lt;- sample(1:60000,1) # Base R, notice that the data is in trainSet$x where each digit is stored as a row vector image(matrix(trainSet$x[randDig,],28,28)[,c(28:1),drop=FALSE],main=paste(trainSet$y[randDig]),asp=1) # ggplot function from the mnistr package, we can supply the data as a row vector mnistr::ggDigit(trainSet$x[randDig,]) # Construct a network with two hidden layers (100 and 50 units) and rectified linear units mnw \u0026lt;- mlp(structNet = c(784,100,50,10), minibatchSize = 100, activation = reLU, initPos = TRUE, initScale = 100) # Define the training set trainX \u0026lt;- trainSet$x trainY \u0026lt;- class.ind(factor(trainSet$y)) # Call train mnw$train(trainData = trainX/256, trainLabels = trainY, num_epochs = 3, eta = 0.005, cvSplit = 0.3)  No hidden layers I wanted to visualize the training when there were no hidden layers. The weights in the gif below correspond to the training over one epoch, where the weights according to the digits are aligned like the matrix below: $$ \\begin{bmatrix} 2 \u0026amp; 5 \u0026amp; 8\\\n1 \u0026amp; 4 \u0026amp; 7\\\n0 \u0026amp; 3 \u0026amp; 6 \\end{bmatrix} $$\n   You can clearly see that the weights resemble templates that evolve to capture more and more of the variation present for each digit. The template also counts for making the patch dissimilar to the digits that it is not trying to match. This simple linear approach achieves a little more than 90% accuracy, which is amazing for how simple the method is. But still, for reading digits on checks, having every tenth read wrong is far from acceptable.\nDecomposing the network with magrittr Now, one of the main things I wanted to achieve with this implementation was to decompose the network into it\u0026rsquo;s individual components, to demonstrate how simple it truly is. Now I can in a simple manner compose functions that I have trained in order to produce some output. The magrittr is perfect for this, because then we do not need to nest inside parenthesis the composition of functions, we can just pipe the input through all the layers.\n# Needed for pipes library(magrittr) # Get the weight matricies from the network we trained above w1 \u0026lt;- mnw$network[[1]]$W$getter() w2 \u0026lt;- mnw$network[[2]]$W$getter() w3 \u0026lt;- mnw$network[[3]]$W$getter() set.seed(12345) randDig \u0026lt;- sample(1:60000,1) # sample random digit testDigit \u0026lt;- matrix(trainSet$x[randDig,]/256,nrow=1,ncol=28*28) # Check label, it is a 2 for this seed trainSet$y[randDig] mnistr::ggDigit(testDigit) # Add the bias term testDigit \u0026lt;- cbind(testDigit,matrix(1,1,1)) # We are passing a single instance simple_softmax \u0026lt;- function(X){ Z \u0026lt;- sum(exp(X)) X \u0026lt;- exp(X)/Z return(X) } # Now pipe it throught the network: # Multiply by weights, use activation and then add 1 for bias testDigit %\u0026gt;% multiply_by_matrix(y=w1) %\u0026gt;% reLU %\u0026gt;% cbind(.,matrix(1,1,1)) %\u0026gt;% multiply_by_matrix(y=w2) %\u0026gt;% reLU %\u0026gt;% cbind(.,matrix(1,1,1)) %\u0026gt;% multiply_by_matrix(y=w3) %\u0026gt;% simple_softmax %\u0026gt;% barplot  The network I trained is 99.6% certain that this is a two! The magrittr package allows us to write out the calculations happening inside the MLP in a very understandable way. Note that some networks used today have more than 50 layers, then this is not so useful anymore. In the case of 50plus layers we need other tricks to train it, e.g. skip connections and other optimizers.\nCode for generating the gifs If you want to create you own gifs, it is probably easiest to define your own mlp function, just look at the mnistr implementation. Then add something similar to the following where the main update is happening. You could of course also save the weights to a matrix outside the environment using the \u0026lt;\u0026lt;- scope assignment operator.\n# Where we do the SGD steps in the train function part of mlp preds \u0026lt;- forward_prop(tDat) backwards_prop(preds,tLab) update_weights(eta = eta) # The following is code for generating plots layerMat \u0026lt;- layers[[1]]$W$getter() # Removw bias layerMat \u0026lt;- layerMat[-nrow(layerMat),] # Set the number of columns and rows for the image nr \u0026lt;- 10 nc \u0026lt;- 10 weightsIm \u0026lt;- matrix(0,28*nr,28*nc) w \u0026lt;- 1 # Fill the image with the relevant weights for(i in 1:nr){ for(m in 1:nc){ weightsIm[((i-1)*28+1):(i*28),((m-1)*28+1):(m*28)] \u0026lt;- matrix(layerMat[,w],28,28)[,c(28:1),drop=FALSE] w \u0026lt;- w + 1 } } # Save for every fourth minibatch if(j%%4 == 1){ data \u0026lt;- c(weightsIm) # Standardize to remove flickering nData \u0026lt;- qnorm(seq(0,1,len=28*28*nr*nc))[rank(data)] nIm \u0026lt;- matrix(nData,28*nr,28*nc) png(paste0('./plots/','reLU',sprintf(\u0026quot;%08d\u0026quot;, k),'.png'),width=28*nc*2,height=28*nr*2) # Remove margins so it looks nicer par(mar = rep(0, 4)) image(nIm,asp=1,col=gray(seq(0,1,by=0.005),1),axes=FALSE) dev.off() k \u0026lt;- k+1 }  Next post For the next post I plan to compare different activation functions and implement dropout for the network. It will probably be the final post until I get new ideas for stuff to implement.\nTweet\n","date":1513861200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513861200,"objectID":"1489d072e7426c97fe219227f4a2fabb","permalink":"https://gumeo.github.io/post/part-2-deep-learning/","publishdate":"2017-12-21T13:00:00Z","relpermalink":"/post/part-2-deep-learning/","section":"post","summary":"Let\u0026rsquo;s go on! If you just arrived, you can check out the first part here. The goal of this series is to demonstrate how compactly we can implement an MLP in a functional programming paradigm and how easy it becomes to extend/play around with it.","tags":["R"],"title":"Part 2: Deep Learning with Closures in R","type":"post"},{"authors":["GuÃ°mundur Einarsson"],"categories":["Programming"],"content":"Start of a small series The gif below is the evolution of the weights from a neural network trained on the mnist dataset. Mnist is a dataset of handwritten digits, and is kind of the hello world/FizzBuzz of machine learning. It is maybe not a very challenging data set, but you can learn a lot from it. This is a 10 by 10 grid of images, where each individual small patch represents weights going to a single neuron in the first hidden layer of the network. After I saw the content by Grant Sanderson, I wanted to inspect by myself what the network is actually learning. This series is going to outline this development, with an angle towards functional programming.\n   So I received the Deep Learning book a little more than a month ago, and I have had time to read parts I and II. I think that overall the authors successfully explain and condense a lot of research into something digestable. The reason why I use the word condense is because how much information the book contains. The bibliography is 55 pages, so I almost feel that the book should be called Introduction to Deep Learning Research, because it is a gateway to so much good material. Another fascinating thing about this book is the discussion it contains. The authors are quite upfront about some criticisms on deep learning and discuss them to a great extent. All in all I look forward to finish reading the book.\nI have fiddled around with deep learning since I took a summer-school on it in 2014, where Hugo Larochelle was giving lectures and he joined us for an epic sword fighting competition/LARP session in the countryside of Denmark. I have followed the evolution of deep learning since, and what has been most amazing is by how much the barrier of entry has been lowered. The current software frameworks make it so easy to get started. After reading the DL book I found a strong inner urge to implement some of these things myself. I think that a way to get a better understanding of programming concepts, algorithms and data structures, is just to go for it and implement it. Also talking about data-structures, deep learning is also becoming a part of that:\n Jeff Dean and co at GOOG just released a paper showing how machine-learned indexes can replace B-Trees, Hash Indexes, and Bloom Filters. Execute 3x faster than B-Trees, 10-100x less space. Executes on GPU, which are getting faster unlike CPU. Amazing. https://t.co/PPVkrFVKXg\n\u0026mdash; Nick Schrock (@schrockn) December 11, 2017   After seeing the videos made by Grant Sanderson on deep learning I decided it was time to implement the basics by myself. I wanted to completely understand back-propagation, how it really is a dynamic programming algorithm where we actually do some smart book-keeping and reuse computations. That is one of the major tricks that makes this algorithm work.\nNot another framework But of course implementing a fully fledged DL framework is a feat one should not do, unless you have some very specific reason for it. Many frameworks have been created, and doing this alone to learn should not have that goal in mind. I wanted to make something that would be easily extendable, I also wanted to do it in R (because I really like R), I also made a package at some point called mnistr where I wanted to add some fun examples with neural networks on. When I\u0026rsquo;m done with this series I\u0026rsquo;ll add the implementation to the package and submit it to CRAN.\nClosures The final reason I had for doing this relates to closures. So deep learning, or deep neural networks, are in it\u0026rsquo;s essance just functions, or rather compositions of functions. These individual functions are usually not that complicated, what makes them complicated is what they learn from complicated data, and how these individual simple things together make something complicated. We do not completely understand what they learn or do. If I can make a representation of a multi-layer percepteron (which is a fully connected neural network) in a functional programming paradigm, then it might be easier to understand it for others and myself. I will hopefully be able to disentanlge the networks into inidvidual functions and using the magrittr package to do the function composition in a more obvious manner, which demonstrates that the individual pieces of a neural network are not so complicated, and the final composition does not need to be so complicated either.\nBut wait a minute? What are closures? So the wikipedia article gives it a pretty good treatment\n In programming languages, closures (also lexical closures or function closures) are techniques for implementing lexically scoped name binding in languages with first-class functions. Operationally, a closure is a record storing a function[a] together with an environment:[1] a mapping associating each free variable of the function (variables that are used locally, but defined in an enclosing scope) with the value or reference to which the name was bound when the closure was created.[b] A closureâ€”unlike a plain functionâ€”allows the function to access those captured variables through the closure\u0026rsquo;s copies of their values or references, even when the function is invoked outside their scope\n This sounds a lot like object oriented programming, but when the objects that we are working with, are naturally functions, then this works very nicely. But let\u0026rsquo;s look at a simple example!\nnewBankAccount \u0026lt;- function(){ myMoney \u0026lt;- 0 putMonyInTheBank \u0026lt;- function(amount){ myMoney \u0026lt;\u0026lt;- myMoney + amount } howMuchDoIOwn \u0026lt;- function(){ print(paste('You have:',myMoney,'bitcoins!')) } return(list2env(list(putMonyInTheBank=putMonyInTheBank, howMuchDoIOwn=howMuchDoIOwn))) }  Now I can use this to create a bank account function\n\u0026gt; myAccount \u0026lt;- newBankAccount() \u0026gt; myAccount$howMuchDoIOwn() [1] \u0026quot;You have: 0 bitcoins!\u0026quot; \u0026gt; myAccount$putMonyInTheBank(200) \u0026gt; myAccount$howMuchDoIOwn() [1] \u0026quot;You have: 200 bitcoins!\u0026quot; \u0026gt; copyAccount \u0026lt;- myAccount \u0026gt; copyAccount$howMuchDoIOwn() [1] \u0026quot;You have: 200 bitcoins!\u0026quot; \u0026gt; copyAccount$putMonyInTheBank(300) \u0026gt; copyAccount$howMuchDoIOwn() [1] \u0026quot;You have: 500 bitcoins!\u0026quot; \u0026gt; myAccount$howMuchDoIOwn() # Ahh snap, I also received bitcoins! [1] \u0026quot;You have: 500 bitcoins!\u0026quot;  So compared to normal functions, now we have a function (actually an environment) with a mutable state. Now we can have side-effects when we call the functions from the environment. But if you look at the calls I made above, you may have noticed that when I copied the account, added to the copied account, the original account also had an increased amount in it. So the copied account didn\u0026rsquo;t get the data copied, only the references. So if you make multiple bank accounts, initialize each seperately, don\u0026rsquo;t initialize one prototype and copy it to all the users. Otherwise all the users end up with one big shared account!\nSo be careful, and if you really want to copy environments look at this SO post. If you want to learn more about environments and the functional programming side of R, advanced R by Hadley Wickham is a great starting point, you might also want to check out this blogpost. Also if you have coded in javascript, you might be familiar with the issue of copying closures there. And btw, I have no bitcoins :(\nAnother important thing that you might have noticed is the assignment operators. If you are not familiar with R, \u0026lt;- is pretty much the same as =, they are kind of used interchangably, but they have a very subtle difference that you can read about here. The weird assignment operator that you noticed was probably the \u0026lt;\u0026lt;-, the scoping assignment. This is the whole trick about closures, best said here:\n A closure is a function written by another function. Closures are so called because they enclose the environment of the parent function, and can access all variables and parameters in that function.\n The scoping operator creates the reference needed, such that the returned function encloses the environment of the caller. This is why closures are sometimes called poor man\u0026rsquo;s objects. We are basically emulating the creation of public and private members in some sense, but without the overhead of object oriented structured code. This is an essential part in making the implementation look cleaner and more transparent. The code that you write is more to the point of solving the task as hand, rather the adhearing to a structure of a particular paradigm.\nToo the point on deep learning again! For the structure of the implementation I was very inspired by this post by Brian Dolhansky, where he implements an MLP in python.\nThis structure completely encapsulates what is needed in a layer and how things are connected. Instead of creating a class, I am going to make a closure. So when I say a layer, I mean the activations from the previous layers and the outgoing weights. So the only layer that doesn\u0026rsquo;t have, or doesn\u0026rsquo;t need weights, is the output layer.\nThis closure is therefore a function, or has some functions, which makes sense for a layer in a neural network, which is essentially a function in the mathematical sense. But before we get to the layer, we need what essentially creates the closure, a building block for a matrix:\nmatrixInLayer \u0026lt;- function(init = FALSE, rS, cS, initPos = FALSE, initScale = 100){ intMat \u0026lt;- matrix(0, nrow=rS, ncol=cS) if(init == TRUE){ intMat \u0026lt;- matrix(rnorm(rS*cS)/initScale,nrow = rS,ncol = cS) if(initPos == TRUE){ intMat \u0026lt;- abs(intMat) } } getter \u0026lt;- function(){ return(intMat) } setter \u0026lt;- function(nMat){ intMat \u0026lt;\u0026lt;- nMat } return(list2env(list(setter = setter, getter=getter))) }  We parameterize this function such that we can account for different initialization strategies in the weights, but we can use this to create all the needed matricies in a layer. The layer closure is then just something that encapsulates the internal state of the network, with placeholders for all the data needed to do a forward and backwards pass. The essential trick to make this work is of course the scope assignment in the setter function. The fully connected layer can now be implemented as:\nLayer \u0026lt;- function(activation, minibatchSize, sizeP, is_input=FALSE, is_output=FALSE, initPos, initScale){ # Matrix holding the output values Z \u0026lt;- matrixInLayer(FALSE,minibatchSize,sizeP[1]) # Outgoing Weights W \u0026lt;- matrixInLayer(TRUE,sizeP[1],sizeP[2],initPos=initPos, initScale=initScale) # Input to this layer S \u0026lt;- matrixInLayer(FALSE,minibatchSize,sizeP[1]) # Deltas for this layer D \u0026lt;- matrixInLayer(FALSE,minibatchSize,sizeP[1]) # Matrix holding derivatives of the activation function Fp \u0026lt;- matrixInLayer(FALSE,sizeP[1],minibatchSize) # Propagate minibatch through this layer forward_propagate \u0026lt;- function(){ if(is_input == TRUE){ return(Z$getter()%*%W$getter()) } Z$setter(activation(S$getter())) if(is_output == TRUE){ return(Z$getter()) }else{ # Add bias for the hidden layer Z$setter(cbind(Z$getter(),rep(1,nrow(Z$getter())))) # Calculate the derivative Fp$setter(t(activation(S$getter(),deriv = TRUE))) return(Z$getter()%*%W$getter()) } } # Return a list of these functions myEnv \u0026lt;- list2env(list(forward_propagate=forward_propagate, S = S, D = D, Fp = Fp, W = W, Z = Z)) class(myEnv) \u0026lt;- 'Layer' return(myEnv) }  The layer function includes all the things needed for doing the book-keeping of the calculations for backpropagation. With very little extra code we have the ability to have arbitrary minibatch sizes and arbitrary activation functions. The activation function just needs and extra boolean parameter to determine whether we are evaluating the function or calculating the deivative. (I\u0026rsquo;ll go in more detail in the next post about what a minibatch is when I cover stocastic gradient descent). We can protype these activation functions on the fly, because R is a functional language. So in less than 50 lines of code we have already created the heart of a multilayer percepteron, with some generalizability. So what does this layer do? It takes input from the activations of the neurons in the previous layer as a vector, multiplies it with a matrix and element-wise applies the activation. In essance it is this: $$ \\sigma(\\mathbf{W}\\cdot \\mathbf{x}) $$ where $\\mathbf{W}$ is the weight matrix, $\\mathbf{x}$ is the input and $\\sigma$ is the activation function. The problem of deep learning is then to find good parameters for the weights $\\mathbf{W}$ when these functions are composed.\nNext post This ended up being a lot longer than I anticipated, but for the next post I aim at finishing the implementation for the MLP, going through backpropagation and simple training on mnist. For the last post the goal is to show how something like dropout can very easily be incorporated in this implementation and how we can disentangle a network using the magrittr package. Implementing other kinds of layers and different optimization strategies is also on the drawing board.\nIf you like this post I would greatly appreciate a tweet, my twitter handle is @gumgumeo :)\nTweet  ","date":1513256400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513256400,"objectID":"523ce6fc5bdf197de80df8aa6ce2549d","permalink":"https://gumeo.github.io/post/part-1-1deep-learning/","publishdate":"2017-12-14T13:00:00Z","relpermalink":"/post/part-1-1deep-learning/","section":"post","summary":"Start of a small series The gif below is the evolution of the weights from a neural network trained on the mnist dataset. Mnist is a dataset of handwritten digits, and is kind of the hello world/FizzBuzz of machine learning.","tags":["R"],"title":"Part 1: Deep Learning with Closures in R","type":"post"},{"authors":["GuÃ°mundur Einarsson"],"categories":["Programming"],"content":"What is a strange attractor? The wikipedia article on attractors gives the following definition/explanation:\n An attractor is called strange if it has a fractal structure. This is often the case when the dynamics on it are chaotic, but strange nonchaotic attractors also exist. If a strange attractor is chaotic, exhibiting sensitive dependence on initial conditions, then any two arbitrarily close alternative initial points on the attractor, after any of various numbers of iterations, will lead to points that are arbitrarily far apart (subject to the confines of the attractor), and after any of various other numbers of iterations will lead to points that are arbitrarily close together. Thus a dynamic system with a chaotic attractor is locally unstable yet globally stable: once some sequences have entered the attractor, nearby points diverge from one another but never depart from the attractor.\n So for a dynamical system, which is usually a solution to a system of differential equations, we have some solutions which are called attractors. The attractors, attract a particle, or something moving in the system, meaning that when the particle falls into the attractor, it stays there. A relatable example is an asteroid that is moving in space and then starts orbiting a planet, and is then stuck in orbit, which is an attractor.\nSome simple equations, like the Lorenz system are chaotic. This means that if you change the initial conditions slightly, what you get out after a certain number of steps, (you iterate the solution in steps usually), is going to be vastly different. When scientists model something, this is not a very attractive property. We would like our system to be robust to minor initial perturbations, but this is not always possible. Many physical systems that we model have this chaotic nature, most notably the weather, it is very hard to make weather forecasts far into the future with good accuracy.\nAlthough attractors are commonly associated with solutions of differential equations, they can also appear in general with an iterative system. An iterative system is a function, or a tuple of functions, where we iteratively feed the output in as input and iterate the solution in this way. This is also how you numerically find solutions to differential equations, where no analytical solution exists, or we can\u0026rsquo;t find one.\nIn case I have piqued your interest in dynamical system, I took some lecture notes in a course I took some years ago. These notes are not quite finished, but should give you a rough idea about the basics and where to go from there.\nWhy is this post tagged as Graphics? Ok, so the point is that these attractors often look very nice. You can render them, but it can be a bit heavy computationally. I saw a very cool tweet by Thomas Lin Pedersen where he was rendering particles in a very nice way in R:\n #generative #rstats pic.twitter.com/h4vDmbrOX8\n\u0026mdash; Thomas Lin Pedersen (@thomasp85) September 26, 2017   At that point I was inspired to do some cool visualizations myself, and learn how to make these aesthetically pleasing renderings. I also wanted to be able to make gifs or screen savers with these things.\nI found this post about rendering strange attractors in R, using Rcpp. The Rcpp code for generating the points is actually very fast, what is the bottleneck here is the rendering with ggplot. So if you implement a rendering stack for a gif with this, the development time when you need to tweak many parameters is going to be rather slow.\nThe Clifford attractor So the attractor in the post I linked to above is called a Clifford attractor. What is interesting about the way it is generated is that the shape is controlled by four parameters $a,b,c$ and $d$. The shape is in some sense continuous in these parameters, meaning that if I make small adjustments in the parameters, the shape will not change too much. With this in mind I wanted to make a gif with it, I.e. we make a loop in the parameters space and render images for equally spaced parameters along this curve. For a small enough spacing, this will give me a continuously deforming shape and I will be able to loop it, because the curve in parameter space is a loop.\nNot all parameters give us nice looking attractors, so I need a way to iterate this process fast, for different sets of parameters, to see what works. I talked to some of my colleagues that know about computer graphics, and they said that they way to go forward would be to use openGL. I actually found a tutorial of exactly what I needed here. This is a tutorial on how to render multiple points, with point smoothing and good anti-aliasing, and this guy actually uses the Clifford attractor, nice!. What I was missing was ways to save intermediate plots, then I was just going to use ImageMagick to combine it into a gif.\n The highest voted answer in this SO thread had everything I needed for saving the images. Now I just had to combine these things together!\nAfter saving the images I used the bash command mogrify to change them into png images and finally ImageMagick to create the gif:\nmogrify -format png *.* convert -delay 120 -loop 0 *.png animated.gif  Here you can see the results on imgur:\n Strange attractors   The code is also now on github in case you want to play around with it!\n","date":1511355600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1511355600,"objectID":"d2abf33cdb47b24299c4ca69afb18cb4","permalink":"https://gumeo.github.io/post/visualizing-strange-attractors/","publishdate":"2017-11-22T13:00:00Z","relpermalink":"/post/visualizing-strange-attractors/","section":"post","summary":"What is a strange attractor? The wikipedia article on attractors gives the following definition/explanation:\n An attractor is called strange if it has a fractal structure. This is often the case when the dynamics on it are chaotic, but strange nonchaotic attractors also exist.","tags":["cpp"],"title":"Visualizing Strange Attractors","type":"post"},{"authors":["GuÃ°mundur Einarsson"],"categories":["Programming"],"content":"I have been waiting to read these for a while now!\n Let the reading commence #rstats #MachineLearning pic.twitter.com/Lqx2IcTENI\n\u0026mdash; Gudmundur Einarsson (@gumgumeo) November 8, 2017   ","date":1510146000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510146000,"objectID":"98a64153be10df760fb147f807491bdf","permalink":"https://gumeo.github.io/post/got-some-new-books/","publishdate":"2017-11-08T13:00:00Z","relpermalink":"/post/got-some-new-books/","section":"post","summary":"I have been waiting to read these for a while now!\n Let the reading commence #rstats #MachineLearning pic.twitter.com/Lqx2IcTENI\n\u0026mdash; Gudmundur Einarsson (@gumgumeo) November 8, 2017   ","tags":["Academic"],"title":"Got some new books!","type":"post"},{"authors":["GuÃ°mundur Einarsson"],"categories":["Demo"],"content":"Starting a blog The plan is to try to do weekly to monthly blogposts about something interesting I learn. I might also write up technical details about setting something up as a reference for my future self.\nI chose to use the blogdown package to manage this blog, with the hugo theme purehugo. The main reason I chose this is because I think it scales well for mobile devices and it looks clean and minimal. I will probably add an about section and some other stuff/fluff later.\nThis is not the first time that I start a blog, I also have my own academic page/blog. I just don\u0026rsquo;t post there regularly. Removing some of the overhead, by managing the content/posts as Rmarkdown documents, will hopefully make me more productive as a blogger.\n","date":1509543600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509543600,"objectID":"2247257f33c5eaa1cd0616bf37b07249","permalink":"https://gumeo.github.io/post/first-post/","publishdate":"2017-11-01T13:40:00Z","relpermalink":"/post/first-post/","section":"post","summary":"Starting a blog The plan is to try to do weekly to monthly blogposts about something interesting I learn. I might also write up technical details about setting something up as a reference for my future self.","tags":["Academic"],"title":"First Post!","type":"post"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"https://gumeo.github.io/about/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]